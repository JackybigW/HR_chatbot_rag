import streamlit as st
from rag_pipeline import LLM_using_RAG, reprompt_agent, retrieve


st.set_page_config(page_title="HRé—®ç­”åŠ©æ‰‹", page_icon="ğŸ’¼")
st.title("AFYä¸“å±äººåŠ›èµ„æºé—®ç­”åŠ©æ‰‹")
st.write("æ¬¢è¿ä½¿ç”¨AFYä¸“å±äººåŠ›èµ„æºé—®ç­”åŠ©æ‰‹ï¼Œå¹´å‡ï¼Œè°ƒä¼‘ï¼Œç¦»èŒï¼Œå…¥èŒï¼Œç­‰ç­‰é—®é¢˜éƒ½å¯ä»¥é—®ï¼")
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.text_input("å¿«æ¥è¯•è¯•å§ï¼")

use_rag = st.checkbox("ä½¿ç”¨å‘˜å·¥æ‰‹å†Œå›ç­”é—®é¢˜", value = True)

if user_input:
    if use_rag:
        answer = LLM_using_RAG(user_input, model = "gpt-5-nano")
    else:
        from openai import OpenAI
        import os
        api_key = os.environ['ZZZ_API_KEY']
        client = OpenAI(api_key=api_key, base_url="https://api.zhizengzeng.com/v1")
        system_prompt = "ä½ æ˜¯ä¼ä¸šHRåŠ©æ‰‹ï¼Œå›ç­”å‘˜å·¥å…³äºå…¬å¸è§„åˆ™çš„é—®é¢˜ã€‚"
        user_prompt = f"ç”¨æˆ·é—®é¢˜: {user_input}\nè¯·æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”ï¼Œä¸ä½¿ç”¨ä»»ä½•å‚è€ƒèµ„æ–™ã€‚"
        response = client.chat.completions.create(
            messages=[
                {"role":"system", "content":system_prompt},
                {"role":"user", "content":user_prompt}
            ],
            model="gpt-5-nano"
        )
        answer = response.choices[0].message.content
    st.session_state.chat_history.append({"user": user_input, "bot": answer})

# Display chat history
for chat in st.session_state.chat_history:
    st.markdown(f"**ä½ :** {chat['user']}")
    st.markdown(f"**åŠ©æ‰‹:** {chat['bot']}")

